# Code Format Fixes Applied âœ…

## Problem Identified

The model was generating code in the wrong format:
- Generated `class Solution` instead of the required function/class name
- Added test code (`if __name__ == '__main__'`) that shouldn't be there
- Didn't follow the exact function signature from the problem

## Root Cause

The stage 5 prompt didn't include the function signature, so the model had to guess the format.

## Fixes Applied

### 1. Updated Stage 5 Prompt (`reasoning/stages.py`)
- Added `{function_signature}` placeholder
- Explicit instruction: "Use the EXACT function/class signature shown above"
- Explicit instruction: "Do NOT add any test code or if __name__ == '__main__' blocks"

### 2. Updated Generator (`models/generator.py`)
- Added `function_signature` parameter to `generate_stage_output()`
- Added `function_signature` parameter to `generate_code()`
- Handles templates with/without the placeholder gracefully

### 3. Updated All Callers
- `benchmark_improvement.py` - passes `problem.function_signature`
- `inference/inference_engine.py` - passes `function_signature`
- `training/adversarial_trainer.py` - passes `problem.function_signature` (3 locations)

### 4. Improved Code Cleaning (`models/generator.py`)
- Better markdown extraction (handles unclosed blocks)
- Removes explanatory text before/after code
- Ensures code starts with `def`, `class`, `import`, or `@`

## Expected Result

Now the model will generate:
```python
def compress_string(s: str) -> str:
    # actual implementation
    return result
```

Instead of:
```python
class Solution:
    def compress(self, s: str) -> str:
        # wrong implementation
        return s

if __name__ == '__main__':
    # test code that breaks execution
    print(...)
```

## Ready to Run!

The benchmark should now work correctly. Run:
```batch
run_cpu_benchmark_auto.bat
```
